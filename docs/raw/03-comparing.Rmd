---
output:
  word_document:
    fig_height: 6
    fig_width: 8
    reference_docx: "../template/WordTemplate_EasyR_Text.docx"
    toc: true
    toc_depth: 2
toc-title: "목차"
editor_options:
  chunk_output_type: console
---

```{r, include=FALSE}
knitr::knit_child("../rmd_etc/rmd_options.Rmd")

library(showtext)
font_add_google(name = "Nanum Gothic", family = "nanumgothic")
showtext_auto()

# 출력 결과 제한 함수
source(here::here("rmd_etc/func_chunk_print_limit.R"))

```


# 3. 텍스트 비교하기

하나의 텍스트만 분석할 수도 있지만 두 텍스트의 차이가 무엇인지 알아보기 위해 텍스트를 비교하는 분석을 하기도 합니다. 단어 빈도 분석을 응용하여 두 텍스트가 강조한 내용이 어떻게 다른지 비교할 수 있습니다.

## 3.1 단어 빈도 비교하기

### 3.1.1 텍스트 합치기

텍스트를 비교하려면 우선 여러 개의 텍스트를 하나의 데이터셋으로 합쳐야 합니다. 문재인 대통령과 박근혜 전 대통령의 대선 출마 선언문을 하나로 합친 다음 텍스트를 비교 분석하는 방법을 알아보겠습니다.


#### 데이터 불러오기

두 연설문 데이터를 불러와 tibble 구조로 변환하고, 어떤 연설문인지 알 수 있게  `mutate()`로 대통령 이름을 부여하겠습니다.

```{r eval=FALSE}
library(dplyr)

# 문재인 대통령 연설문 불러오기
raw_moon <- readLines("speech_moon.txt", encoding = "UTF-8")
moon <- raw_moon %>%
  as_tibble() %>%
  mutate(president = "moon")

# 박근혜 대통령 연설문 불러오기
raw_park <- readLines("speech_park.txt", encoding = "UTF-8")
park <- raw_park %>%
  as_tibble() %>%
  mutate(president = "park")
```

```{r echo=F}
library(dplyr)

# 문재인 대통령 연설문 불러오기
raw_moon <- readLines(here::here("files/speech_moon.txt"), encoding = "UTF-8")

moon <- raw_moon %>%
  as_tibble() %>%
  mutate(president = "moon")

# 박근혜 대통령 연설문 불러오기
raw_park <- readLines(here::here("files/speech_park.txt"), encoding = "UTF-8")

park <- raw_park %>%
  as_tibble() %>%
  mutate(president = "park")
```


#### 데이터 합치기

`bind_rows()`를 이용해 두 데이터를 행(세로) 방향으로 결합하고, 출력했을 때 알아보기 편하게 `select()`로 변수 순서를 바꾸겠습니다. 코드의 출력 결과를 보면 `bind_speeches`의 윗부분은 문재인 대통령 연설문, 아랫부분은 박근혜 대통령 연설문으로 구성되어 있습니다.

```{r}
bind_speeches <- bind_rows(moon, park) %>%
  select(president, value)

head(bind_speeches)
tail(bind_speeches)
```

- 박근혜 전 대통령 대선 출마 선언문 출처

[ko.wikisource.org/wiki/새누리당_박근혜_대한민국_제18대_대통령_선거_출마_선언문](https://ko.wikisource.org/wiki/새누리당_박근혜_대한민국_제18대_대통령_선거_출마_선언문)


### 3.1.2 집단별 단어 빈도 구하기

이제 결합한 데이터를 토큰화해 빈도를 구하고, 두 연설문에서 어떤 단어가 많이 사용되었는지 비교하겠습니다.


#### 1. 기본적인 전처리 및 토큰화

연설문에서 한글 이외의 문자와 연속된 공백을 제거하고 형태소 분석기를 이용해 명사 기준으로 토큰화하겠습니다.  1장과 2장에서는 전처리 과정에 문자열 벡터를 이용했기 때문에 변수에 `str_replace_all()`과 `str_squish()`를 바로 적용했습니다. 여기서는 tibble 구조의 `bind_speeches`에 들어 있는 `value` 변수를 변환해야 하므로 `mutate()`를 이용합니다.

```{r}
# 기본적인 전처리
library(stringr)
speeches <- bind_speeches %>%
  mutate(value = str_replace_all(value, "[^가-힣]", " "),
         value = str_squish(value))

speeches

# 토큰화
library(tidytext)
library(KoNLP)

speeches <- speeches %>%
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)
speeches
```


#### 2. 집단별 단어 빈도 구하기 - `count()`

`speeches`의 `president` 변수는 `"moon"`과 `"park"`으로 구분되어 있습니다. 두 연설문에서 각각 어떤 단어가 많이 사용되었는지 알아봐야 하기 때문에 `"moon"`과 `"park"`에 포함된 단어 빈도를 따로 구해야 합니다.

#### 샘플 텍스트로 작동 원리 알아보기

`count()`를 이용하면 하위 집단별 빈도를 구할 수 있습니다. `count()`에 집단을 구성하는 두 변수를 순서대로 입력하면, 데이터를 첫 번째 변수의 항목별로 나눈 뒤, 각 항목에 속한 두 번째 변수의 항목별로 다시 나누어 하위 집단별 빈도를 구합니다.

```{r}
df <- tibble(class = c("a", "a", "a", "b", "b", "b"),
             sex = c("female", "male", "female", "male", "male", "female"))
df

df %>% count(class, sex)
```


#### 두 연설문의 단어 빈도 구하기

이제 `count()`를 이용해 `"moon"`과 `"park"`의 단어 빈도를 구한 다음 두 글자 이상의 단어만 남기겠습니다. 출력 결과를 보면 두 연설문에서 어떤 단어가 몇 번씩 사용되었는지 알 수 있습니다.

```{r}
frequency <- speeches %>%
  count(president, word) %>%   # 연설문 및 단어별 빈도
  filter(str_count(word) > 1)  # 두 글자 이상 추출

head(frequency)
```

> [참고] `count()`는 입력한 변수의 알파벳, 가나다순으로 행을 정렬합니다.


### 3.1.3 자주 사용된 단어 추출하기 - `slice_max()`

`dplyr`패키지의 `slice_max()`를 이용하면 텍스트에서 자주 사용된 단어를 간편하게 추출할 수 있습니다.

#### 샘플 데이터로 작동 원리 알아보기

`slice_max()`는 변수의 값이 큰 n행을 추출해 내림차순으로 정렬하는 기능을 합니다. 아래 샘플 데이터에 `slice_max()`를 적용한 결과를 보면 `x`가 가장 큰 5행이 내림차순으로 정렬됐음을 알 수 있습니다.

```{r}
df <- tibble(x = c(1:100))
df

df %>% slice_max(x, n = 3)
```

> [참고] 상위 n행을 추출해 내림차순 정렬하는 여러가지 방법이 있지만 `slice_max()`를 이용하는 게 가장 간단합니다.

```{r eval=F}
df %>%
  arrange(desc(x)) %>%
  head(5)

df %>%
  top_n(x, n = 5) %>%
  arrange(desc(x))
```


#### 연설문에서 가장 많이 사용된 단어 추출하기

`slice_max()`를 이용해 각 연설문에서 가장 많이 사용된 단어를 10개씩 추출하겠습니다. `frequency`를 `group_by()`를 이용해 `president` 별로 분리한 다음, `slice_max()`를 이용해 단어 빈도 `n`이 가장 높은 상위 10개 단어를 추출하면 됩니다.

```{r}
top10 <- frequency %>%
  group_by(president) %>%  # president별로 분리
  slice_max(n, n = 10)     # 상위 10개 추출

top10
```

출력 결과물에서 `A tibble: 22 x 3`을 보면 `top10`이 22행으로 구성됨을 알 수 있습니다. 두 연설문에서 빈도가 높은 단어를 10개씩 추출했는데 20행이 아니라 22행인 이유는 빈도가 동점인 행이 전부 함께 추출됐기 때문입니다. 박근혜 대통령 연설문에 `"교육"`, `"사람"`, `"사회"`, `"일자리"`가 똑같이 9번씩 사용돼 빈도가 같기 때문에 전부 추출되어 행이 늘어난 것입니다. 만약 아홉 번 사용된 단어가 이것 말고도 더 있다면 전체 행의 수도 그만큼 늘어났을 것입니다. 다음 코드의 출력 결과를 보면 박근혜 대통령 연설문의 단어는 10개가 아니라 12개입니다.

```{r}
top10 %>%
  filter(president == "park") %>%
  print(n = Inf)
```

> [참고] `print(n = Inf)`는 tibble의 모든 행을 출력하는 기능을 합니다.

### 3.1.4 빈도가 동점인 단어는 제외하고 추출하기 - `slice_max(with_ties = F)`

빈도가 동점인 행을 추출하지 않도록 제한하면 원하는 만큼만 단어를 추출할 수 있습니다. `slice_max()`에 `with_ties = F`를 입력하면 동점이 있을 때 `n`에 입력한 만큼만 행을 추출합니다.

#### 샘플 데이터로 작동 원리 알아보기

샘플 데이터로 `slice_max(with_ties = F)`가 어떻게 작동하는지 알아보겠습니다. 코드의 출력 결과를 보면 동점이 있을 때 데이터가 정렬된 순서에 따라 행을 추출한다는 것을 알 수 있습니다.

```{r}
df <- tibble(x = c("A", "B", "C", "D"), y = c(4, 3, 2, 2))
```

<br>

```{r}
df %>% 
  slice_max(y, n = 3)
```

<br>

```{r}
df %>% 
  slice_max(y, n = 3, with_ties = F)
```


<!-- ```{r} -->
<!-- df <- tibble(x = c("A", "B", "C", "D"), y = c(4, 3, 2, 2)) -->
<!-- df %>% slice_max(y, n = 3, with_ties = F) -->

<!-- df <- tibble(x = c("D", "C", "B", "A"), y = c(4, 3, 2, 2)) -->
<!-- df %>% slice_max(y, n = 3, with_ties = F) -->
<!-- ``` -->

> [편집] 두 코드 블럭 다단 편집

#### 연설문에 적용하기

연설문에서 단어 빈도 동점을 제외하고 가장 많이 사용된 단어를 추출하겠습니다. 코드의 출력 결과에서 `A tibble: 20 x 3`을 보면 `top10_notie`가 20행임을 알 수 있습니다.

```{r}
top10 <- frequency %>%
  group_by(president) %>%
  slice_max(n, n = 10, with_ties = F)
```


```{r eval=F}
top10
```

```{r echo=F}
top10 %>% print(n = 10)
```

### 3.1.4 막대 그래프 만들기

두 대통령이 어떤 단어를 많이 사용했는지 쉽게 비교할 수 있게 연설문별로 막대 그래프를 만들어보겠습니다.

#### 1. facet_wrap()을 이용해 항목 별 그래프 한 번에 만들기

`facet_wrap()`은 변수의 항목별로 그래프를 생성합니다. `~` 뒤에 입력한 변수가 그래프를 나누는 기준이 됩니다.
```{r}
library(ggplot2)
ggplot(top10, aes(x = reorder(word, n),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president)
```

그래프를 보면 축은 있지만 막대는 없는 항목들이 있습니다. 이는 축을 구성하는 단어가 `president` 중 한 곳에만 포함되어 있기 때문입니다. 예를 들어 `top10`에 "행복"은 `park`에는 있지만 `moon`에는 없고, "나라"는 `moon`에는 있지만 `park`에는 없습니다. 항목 자체가 없기 때문에 빈도가 표현되지 않은 것입니다.


#### 2. 그래프별 y축 설정하기

그래프별로 y축을 따로 만들면 이런 문제를 피할 수 있습니다. `facet_wrap()`의 `scales`에 `"free_y"`를 지정하면 그래프별로 y축을 생성합니다. `scales`는 여러 그래프의 축을 통일할 것인지 아니면 각각 생성할 것인지 결정하는 파라미터입니다. 다음 코드로 그래프를 만들면 y축을 `president`별로 각각 생성합니다.

```{r}
ggplot(top10, aes(x = reorder(word, n),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president,         # president별 그래프 생성
              scales = "free_y")  # y축 통일하지 않음
```


#### 3. 특정 단어 제외하고 막대 그래프 만들기

박근혜 대통령의 단어 빈도 그래프를 보면, `"국민"`의 빈도가 너무 높아 단어들의 빈도 차이가 잘 드러나지 않습니다. 전반적인 단어의 빈도가 잘 드러나도록 `"국민"`을 제외하고 가장 많이 사용된 단어 10개를 추출해 그래프를 만들겠습니다.

```{r}
top10 <- frequency %>%
  filter(word != "국민") %>%
  group_by(president) %>%
  slice_max(n, n = 10, with_ties = F)
```


```{r eval=F}
top10
```


```{r echo=F}
top10 %>% print(n = 10)
```


```{r}
ggplot(top10, aes(x = reorder(word, n),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y")
```

#### 4. 축 정렬하기

앞에서 출력한 그래프를 보면 x축을 지정할 때 `reorder(word, n)`를 입력했는데도 막대가 빈도 기준으로 완벽하게 정렬되지 않았습니다. 이는 `facet_wrap()`으로 `president`별 그래프를 생성할 때 `president`별로 구한 빈도가 아니라 전체를 대상으로 구한 빈도를 x축 순서를 정하는 기준으로 삼았기 때문입니다.


#### 그래프별로 축 정렬하기 - `reorder_within()`

x축을 지정할 때 `reorder()` 대신 `tidytext` 패키지의 `reorder_within()`을 적용하면 이 문제를 해결할 수 있습니다. `reorder_within()`은 축 순서를 변수의 항목 별로 각각 구합니다. `reorder_within()`에는 3개의 파라미터를 입력해야 합니다.

`x` : 축

`by` :정렬 기준

`within` : 그래프를 나누는 기준

여기서는 축을 `word`로 하고, 정렬 기준은 단어 빈도 `n`, 그래프를 나누는 기준은 `president`로 설정하겠습니다. 코드를 짧게 만들기 위해 파라미터명은 생략하겠습니다.

```{r}
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y")
```

#### 5. 변수 항목 제거하기 - `scale_x_reordered()`

그래프를 보면 각 단어 뒤에 `president`의 항목인 `moon`, `park`이 붙어있습니다. 단어가 어떤 `president`에 속하는지 표시한 것입니다. 이 값을 표시하지 않으려면 `tidytext` 패키지의 `scale_x_reordered()`를 추가하면 됩니다. 

```{r}
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y") +
  scale_x_reordered() +
  labs(x = NULL) +                                    # x축 삭제
  theme(text = element_text(family = "nanumgothic"))  # 폰트
```



## 3.2 오즈비 - 상대적으로 중요한 단어 비교하기

지금까지 단어 빈도를 살펴보는 방식으로 텍스트를 비교했습니다. 그런데 어떤 텍스트든 일반적으로 많이 사용하는 단어의 빈도가 높기 때문에 이런 방식으로 비교하면 텍스트의 차이가 잘 드러나지 않는 한계가 있습니다.

예를 들어 앞에서 살펴본 두 연설문에서 가장 많이 사용된 단어 10개에 `"우리"`, `"사회"`, `"경제"`, `"일자리"`가 공통적으로 포함됩니다. 이와 같이 단순히 빈도가 높은 단어는 보편적으로 많이 사용되고 별다른 특징이 없기 때문에 텍스트의 차이를 잘 드러내지 못합니다. 텍스트를 비교할 때는 특정 텍스트에는 많이 사용됐지만 다른 텍스트에는 적게 사용된 단어를 살펴봐야 합니다. 상대적으로 많이 사용된 단어를 살펴봐야 텍스트의 차이를 제대로 이해할 수 있습니다.


### 3.2.1 Long form을 Wide form으로 변환하기

앞에서 생성한 `frequency`는 `president`가 `"moon"`인 데이터와 `"park"`인 데이터가 세로로 길게 나열된 형태입니다. 이렇게 세로로 길게 나열된 데이터 형태를 'Long form'이라고 합니다.

#### Long form 데이터 살펴보기

`frequency`에서 일부를 추출해 Long form 데이터의 특징을 살펴보겠습니다. 다음 코드의 출력 결과를 보면 `"국민"`, `"우리"`는 두 연설문에 모두 포함된 반면, `"정치"`는 `"moon"`에만, `"행복"`은 `"park"`에만 포함되어 있습니다. 이처럼 Long form 데이터는 같은 단어가 범주 별로 다른 행을 구성하기 때문에 단어가 각 범주에서 몇 번씩 사용됐는지 비교하기 어렵고, 단어 빈도를 활용해 연산하기도 불편합니다.

```{r}
df_long <- frequency %>%
  group_by(president) %>%
  slice_max(n, n = 10) %>%
  filter(word %in% c("국민", "우리", "정치", "행복"))
```


```{r eval=F}
df_long
```
```{r echo=F}
df_long %>% print(n = Inf)
```


#### Long form을 Wide form으로 변형하기 - `pivot_wider()`

단어가 두 연설문에서 몇 번씩 사용되었는지 비교하고 변수 간 연산을 하기 쉽도록 Long form 데이터를 가로로 넓은 형태의 Wide form 데이터로 변형하겠습니다. `tidyr` 패키지의 `pivot_wider()`를 이용하면 됩니다. `pivot_wider()`에는 두 가지 파라미터를 지정해야 합니다.

- `names_from` : 어떤 변수의 값으로 새 데이터의 변수를 만들지 지정합니다. 여기서는 `president`에 들어 있는 `"moon"`과  `"park"`을 각각 변수로 만들 것이므로 `president`를 입력하면 됩니다.

- `values_from` : 어떤 변수의 값으로 새 데이터의 변수를 채울지 지정합니다. 여기서는 단어 빈도를 채워 넣어야 하므로 `n`을 입력합니다.

다음 코드의 출력 결과를 보면 한 행이 한 단어로 된 Wide form이기 때문에 어떤 단어가 두 연설문에서 몇 번 사용되었는지 쉽게 비교할 수 있습니다.

```{r eval=F}
install.packages("tidyr")
library(tidyr)

df_wide <- df_long %>%
  pivot_wider(names_from = president,
              values_from = n)

df_wide
```

```{r echo=F}
# install.packages("tidyr")
library(tidyr)

df_wide <- df_long %>%
  pivot_wider(names_from = president,
              values_from = n)

df_wide
```

#### `NA`를 `0`으로 바꾸기

앞에서 실행한 코드의 결과물을 보면 결측치 `NA`가 포함되어 있습니다. 어떤 단어가 둘 중 한 연설문에만 있으면 `n`의 값이 없기 때문에 `NA`가 됩니다. 값이 `NA`면 연산할 수 없으므로 `0`으로 변환해야 합니다. `pivot_wider()`의 `values_fill`에 `list(n = 0)`을 입력하면 `NA`를 `0`으로 변환해 줍니다.

```{r}
df_wide <- df_long %>%
  pivot_wider(names_from = president,
              values_from = n,
              values_fill = list(n = 0))

df_wide
```


#### 연설문 단어 빈도를 Wide form으로 변환하기

Long form 데이터를 Wide form으로 변환하는 방법을 익혔으니 연설문의 단어 빈도가 들어 있는 `frequency`를 `Wide form`으로 변환하겠습니다. 다음 코드를 실행하면 데이터가 Wide form으로 변환되어 단어 빈도를 쉽게 비교할 수 있습니다.

```{r}
frequency_wide <- frequency %>%
  pivot_wider(names_from = president,
              values_from = n,
              values_fill = list(n = 0))

frequency_wide
```



### 3.2.2 오즈비 구하기

**오즈비(Odds ratio)**는 어떤 사건이 A 조건에서 발생할 확률이 B 조건에서 발생할 확률 보다 얼마나 더 큰지 나타낸 값입니다. 단어 빈도를 이용해 오즈비를 구하면 단어가 두 텍스트 중 어디에 등장할 확률이 높은지, 상대적인 중요도를 알 수 있습니다.


#### 1. 단어의 비중을 나타낸 변수 추가하기

`frequency_wide`를 이용해 오즈비를 구하는 과정을 단계별로 살펴보겠습니다. 먼저, 각 단어가 두 연설문에서 차지하는 비중이 얼마인지 나타낸 변수를 추가하겠습니다. 연설문별로 '각 단어의 빈도'를 '모든 단어의 빈도 합'으로 나누면 됩니다.

특정 연설문에서 단어가 한 번도 사용되지 않아 빈도가 0이면 오즈비가 0이므로 어떤 연설문에서 비중이 높은지 알 수 없게 됩니다. 이런 문제를 피하려면 빈도가 0보다 큰 값이 되도록 모든 값에 `1`을 더하면 됩니다.

```{r}
frequency_wide <- frequency_wide %>%
  mutate(ratio_moon  = ((moon + 1)/(sum(moon + 1))),  # moon에서 단어의 비중
         ratio_park  = ((park + 1)/(sum(park + 1))))  # park에서 단어의 비중

frequency_wide
```


#### 2. 오즈비 변수 추가하기

한 텍스트의 단어 비중을 다른 텍스트의 단어 비중으로 나누면 오즈비가 됩니다. 앞에서 구한 `ratio_moon`을 `ratio_park`으로 나누어 각 단어의 비중이 `park`에 비해 `moon`에서 얼마나 더 큰지, 상대적인 비중을 나타낸 오즈비 변수를 추가하겠습니다.

```{r}
frequency_wide <- frequency_wide %>%
  mutate(odds_ratio = ratio_moon/ratio_park)
```

오즈비를 보면 단어가 어떤 텍스트에서 상대적으로 더 많이 사용됐는지 알 수 있습니다. 다음 코드의 출력 결과를 보면 `"moon"`에서 상대적으로 많이 사용된 단어일수록 오즈비 `odds_ratio` 값이 크다는 것을 알 수 있습니다. `odds_ratio`는 단어의 상대적인 비중이 동일하면 1, `"moon"`에서 상대적인 비중이 클수록 1보다 큰 값, 반대로 `"park"`에서 상대적인 비중이 클수록 1보다 작은 값이 됩니다.


```{r echo=F}
options(tibble.width = 60)      # tibble 출력 폭 제한
```


```{r}
frequency_wide %>%
  arrange(-odds_ratio)
```

> [참고] `arrange()`에 입력하는 변수에 `-`를 붙이면 내림차순으로 정렬합니다.

<br>

```{r}
frequency_wide %>%
  arrange(odds_ratio)
```


단어 빈도 오즈비를 수식으로 나타내면 다음과 같습니다. 수식에서 `n`은 각 단어 빈도, `total`은 전체 단어 빈도를 의미합니다.

$$\text{odds ratio} = \frac{\left(\frac{n+1}{\text{total}+1}\right)_\text{Text A}}
                           {\left(\frac{n+1}{\text{total}+1}\right)_\text{Text B}}$$


<!-- 참고 -->
<!-- https://github.com/dgrtwo/tidy-text-mining/blob/master/07-tweet-archives.Rmd -->
<!-- https://www.tidytextmining.com/twitter.html -->


#### 3. 오즈비 간단히 구하기

앞에서는 오즈비의 의미를 설명하기 위해 변수를 한 단계씩 만들었지만 `mutate()`를 이용하면 여러 변수를 동시에 추가할 수 있습니다.

```{r, eval = F}
frequency_wide <- frequency_wide %>%
  mutate(ratio_moon  = ((moon + 1)/(sum(moon + 1))),
         ratio_park  = ((park + 1)/(sum(park + 1))),
         odds_ratio = ratio_moon/ratio_park)
```

오즈비만 필요하다면 다음과 같이 더 간단히 구할 수 있습니다.
```{r, eval = F}
frequency_wide <- frequency_wide %>%
  mutate(odds_ratio = ((moon + 1)/(sum(moon + 1)))/
                      ((park + 1)/(sum(park + 1))))
```


### 3.2.3 상대적으로 중요한 단어 비교하기

오즈비를 구했으니 이제 두 연설문에서 상대적으로 중요한 단어를 추출해 비교하겠습니다.

#### `filter()`와 `rank()`를 이용해  오즈비가 가장 높거나 가장 낮은 단어 추출하기

`filter()`와 `rank()`를 이용해 `odds_ratio` 기준 상위 10개 또는 하위 10개의 단어를 추출한 다음 내림차순으로 정렬해 결과를 살펴보겠습니다. `rank()`는 값이 큰 순으로 순위를 구합니다. 변수명 앞에 `-`를 입력하면 반대로 작은 순으로 순위를 구합니다.

```{r echo=F}
options(tibble.width = 60)      # tibble 출력 폭 제한
```


```{r}
top10 <- frequency_wide %>%
  filter(rank(odds_ratio) <= 10 | rank(-odds_ratio) <= 10)

```

```{r eval=F}
top10 %>%
  arrange(-odds_ratio) %>%
  print(n = Inf)
```

```{r echo=F, output.lines = 9}
top10 %>%
  arrange(-odds_ratio) %>%
  print(n = Inf)
```

```{r echo=F, output.lines = 18:23}
top10 %>%
  arrange(-odds_ratio) %>%
  print(n = Inf)
```

> [편집] ... 중복 삭제, 마지막 ... 삭제


추출한 단어 중 상위 10개는 `"moon"`에서 더 중요하게 사용되어 `odds_ratio`가 높은 단어입니다. `"복지국가"`, `"여성"`, `"공평"`과 같은 단어의 `odds_ratio`를 보면 문재인 대통령이 박근혜 대통령 보다 복지와 평등을 더 강조했음을 알 수 있습니다.

반대로 하위 10개는 `park`에서 더 중요하게 사용되어 `odds_ratio`가 낮은 단어입니다. `"박근혜"`, `"여러분"` 등을 보면 박근혜 대통령이 문재인 대통령 보다 개인의 정체성과 국민과의 유대감을 더 강조했음을 알 수 있습니다.

앞에서 단순히 사용 빈도 기준으로 추출한 단어는 `"국민"`, `"우리"`, `"사회"`와 같은 보편적인 단어라 연설문의 차이가 잘 드러나지 않았습니다. 반면 오즈비 기준으로 추출한 단어를 보면 두 연설문 중 한 쪽에서 비중이 큰 단어들이므로 연설문의 차이가 분명하게 드러납니다.

> [참고] `rank()`는 값의 순위를 구하는 함수입니다. 변수에 `-`를 붙이면 값이 클수록 앞순위가 됩니다.

```{r}
df <- tibble(x = c(2, 5, 10))
df %>% mutate(y = rank(x))     # 값이 작을수록 앞순위
df %>% mutate(y = rank(-x))    # 값이 클수록 앞순위
```


### 3.2.4 막대 그래프 만들기

두 연설문의 주요 단어를 비교하기 쉽게 막대 그래프를 만들겠습니다. 

#### 1. 변수 추가하기

우선 각 단어가 어떤 연설문의 주요 단어인지 나타낸 변수 `president`를 추가하겠습니다. `odds_ratio`가 1보다 크면 `"moon"`, 그렇지 않으면 `"park"`을 부여하겠습니다. 이 변수는 연설문 별로 각각 다른 그래프를 만드는데 활용됩니다. 그런 다음, 주요 연설문의 단어 빈도를 나타낸 변수 `n`을 추가하겠습니다. `odds_ratio`가 1보다 크면 `moon`, 그렇지 않으면 `park`의 값을 가져오면 주요 연설문의 빈도를 채울 수 있습니다.

```{r echo=F}
options(tibble.width = 80)      # tibble 출력 폭 제한
```


```{r}
top10 <- top10 %>%
  mutate(president = ifelse(odds_ratio > 1, "moon", "park"),
         n = ifelse(odds_ratio > 1, moon, park))
```


```{r eval=F}
top10
```


```{r echo=F}
top10 %>% print(n = 10)
```

#### 2. 막대 그래프 만들기

이제 `top10`을 이용해 막대 그래프를 만들겠습니다. 그래프를 보면 전반적으로 단어가 `"park"`에는 많이 사용되었지만 `"moon"`에는 별로 사용되지 않은 것처럼 보입니다. 이는 `"park"`에서 가장 많이 사용된 단어인 `"행복"`의 빈도를 기준으로 두 그래프의 x축 크기를 똑같이 고정했기 때문입니다.

```{r}
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y") +
  scale_x_reordered()
```


#### 3. 그래프별로 축 설정하기

`"moon"` 그래프는 `"moon"`의 단어 빈도 기준으로, `"park"` 그래프는 `"park"`의 단어 빈도 기준으로 x축 크기를 정해야 각 연설문에서 단어의 비중을 제대로 알 수 있습니다. `facet_wrap()`의 `scales`에 `"free"`를 입력하면 y축 크기뿐만 아니라 x축 크기도 그래프별로 따로 정할 수 있습니다.

```{r}
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free") +
  scale_x_reordered() +
  labs(x = NULL) +                                    # x축 삭제
  theme(text = element_text(family = "nanumgothic"))  # 폰트
```


x축 크기가 그래프마다 다르면 막대 길이가 같더라도 실제 값은 다르기 때문에 해석할 때 조심해야 합니다. 오즈비를 이용한 막대 그래프는 단어 빈도를 비교하는 게 아니라 각 텍스트에서 상대적으로 중요한 단어가 무엇인지 표현하기 위해 만듭니다. 막대 길이만 보고 두 텍스트의 단어 빈도를 비교하면 안 되고, 각 텍스트에서 상대적으로 중요한 단어가 무엇인지 살펴보는 용도로 사용해야 합니다.


<!-- #### 4. 그래프 다듬기 -->

<!-- `ggplot2` 패키지 함수를 추가해 그래프를 보기 좋게 수정하겠습니다. 우선 그래프의 축 스케일을 조정할 때 사용할 `scales` 패키지를 설치하고 로드하겠습니다. y축을 정수로 표현하기 위해 `scales` 의 `breaks_pretty()`를 사용하게 됩니다. -->

<!-- ```{r eval=F} -->
<!-- # 축 스케일 수정 패키지 -->
<!-- install.packages("scales") -->
<!-- library(scales) -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- # install.packages("scales") -->
<!-- library(scales) -->
<!-- ``` -->

<!-- 이제 연설문의 범주를 한글로 수정한 다음 그래프를 만들겠습니다. -->

<!-- ```{r} -->
<!-- # 한글로 이름 바꾸기 -->
<!-- top10 <- top10 %>% -->
<!--   mutate(president = ifelse(president == "moon", "문재인", "박근혜")) -->

<!-- ggplot(top10, aes(x = reorder_within(word, n, president), -->
<!--                   y = n, -->
<!--                   fill = president)) + -->
<!--   geom_col(show.legend = F) +                         # 범례 삭제 -->
<!--   coord_flip() + -->
<!--   facet_wrap(~ president, scales = "free") + -->
<!--   scale_x_reordered() + -->
<!--   geom_text(aes(label = n), hjust = -0.3) +           # 빈도 표시 -->
<!--   scale_y_continuous( -->
<!--     expand = expansion(mult = c(0.05, 0.15)),         # y축 막대-그래프 경계 간격 -->
<!--     breaks = breaks_pretty()) +                       # y축 정수 표기 -->
<!--   theme_bw() +                                        # 테마 적용 -->
<!--   theme(text = element_text(family = "nanumgothic"),  # 폰트 -->
<!--         title = element_text(size = 12),              # 제목 크기 -->
<!--         strip.text = element_text(size = 10)) +       # president 이름 크기 -->
<!--   labs(title = "대통령 출마 선언문 주요 단어",        # 제목 -->
<!--        subtitle = "오즈비 기준 사용 빈도 Top 10",     # 부제목 -->
<!--        x = NULL, y = NULL)                            # x, y축 이름 삭제 -->
<!-- ``` -->


### 3.2.5 주요 단어가 사용된 문장 살펴보기

두 연설문에서 상대적으로 중요한 단어가 무엇인지 확인했으니, 이제 단어가 사용된 문장을 살펴보겠습니다.

#### 1. 원문을 문장 기준으로 토큰화하기

먼저 두 연설문의 원문이 들어있는 `bind_speeches`를 문장 기준으로 토큰화하겠습니다. `speeches_sentence`를 출력한 결과를 보면 위쪽은 `"moon"`, 아래쪽은 `"park"`의 연설문 문장이 각 행으로 구성됩니다.


```{r echo=F}
options(tibble.width = 47)      # tibble 출력 폭 제한
```


```{r}
speeches_sentence <- bind_speeches %>%
  as_tibble() %>%
  unnest_tokens(input = value,
                output = sentence,
                token = "sentences")

head(speeches_sentence)
tail(speeches_sentence)
```


#### 2. 주요 단어가 사용된 문장 추출하기 - `str_detect()`

`filter()`와 `str_detect()`를 이용해 각 연설문에서 주요 단어가 사용된 문장을 추출하겠습니다. 다음 코드로 추출한 문장을 보면, 관심 단어가 문장에서 구체적으로 어떻게 사용되었는지 알 수 있습니다.

```{r eval=F}
speeches_sentence %>%
  filter(president == "moon" & str_detect(sentence, "복지국가"))
```

```{r echo=F}
speeches_sentence %>%
  filter(president == "moon" & str_detect(sentence, "복지국가")) %>% 
  print(n = 10)
```

<br>

```{r eval=F}
speeches_sentence %>%
  filter(president == "park" & str_detect(sentence, "행복"))
```

```{r echo=F}
speeches_sentence %>%
  filter(president == "park" & str_detect(sentence, "행복")) %>% 
  print(n = 10)
```


### 3.2.6 상대적인 중요도가 비슷한 단어 살펴보기

이번에는 두 연설문에서 상대적으로 중요도가 비슷한 단어를 살펴보겠습니다. `odds_ratio`가 1에 가까운 단어를 추출하면 됩니다. 출력 결과를 보면 대부분 보편적인 의미를 지니는 단어들임을 알 수 있습니다.


```{r echo=F}
options(tibble.width = 80)      # tibble 출력 폭 제한
```


```{r}
frequency_wide %>%
  arrange(abs(1 - odds_ratio)) %>%
  head(10)
```

#### 상대적인 중요도가 비슷하면서 빈도가 높은 단어 추출하기

앞에서 추출한 단어들은 상대적인 중요도가 비슷하지만 빈도가 낮아서 강조한 단어는 아니라고 볼 수 있습니다. 이번에는 두 연설문에서 5번 이상 사용된 단어 중에서 `odds_ratio`가 1에 가까운 단어를 추출하겠습니다. 출력 결과를 보면 두 연설문 모두 `"사회"`, `"사람"`, `"경제"` 등을 강조했음을 알 수 있습니다.

```{r}
frequency_wide %>%
  filter(moon >= 5 & park >= 5) %>%
  arrange(abs(1 - odds_ratio)) %>%
  head(10)
```



## 3.3 로그 오즈비로 단어 비교하기

이번에는 오즈비에 로그를 취한 **로그 오즈비(Log odds ratio)**를 활용하는 방법을 알아보겠습니다. 어떤 값에 로그를 취하면 1보다 큰 값은 양수, 1보다 작은 값은 음수가 됩니다. 따라서 앞에서 구한 오즈비 `odds_ratio`에 로그를 취하면, `moon`에서 비중이 커서 `odds_ratio`가 1보다 큰 단어는 양수가 됩니다. 반대로 `park`에서 비중이 커서 `odds_ratio`가 1보다 작은 단어는 음수가 됩니다.

로그 오즈비는 단어가 어떤 텍스트에서 비중이 큰지에 따라 서로 다른 부호를 갖습니다. 따라서 로그 오즈비를 이용해 막대 그래프를 만들면 반대되는 축 방향으로 단어의 중요도를 표현해 텍스트의 차이를 분명하게 드러낼 수 있습니다. 단어 빈도 로그 오즈비를 수식으로 나타내면 다음과 같습니다.

$$\text{log odds ratio} = \log{\left(\frac{\left(\frac{n+1}{\text{total}+1}\right)_\text{Text A}}
                              {\left(\frac{n+1}{\text{total}+1}\right)_\text{Text B}}\right)}$$


<!-- # 중괄호 -->
<!-- $$\text{log odds ratio} = \log{\left\{\frac{\left(\frac{n+1}{\text{total}+1}\right)_\text{Text A}} -->
<!--                               {\left(\frac{n+1}{\text{total}+1}\right)_\text{Text B}}\right\}}$$ -->

<!-- # 괄호 생략                               -->
<!-- $$\text{log odds ratio} = log\frac{\left(\frac{n+1}{\text{total}+1}\right)_\text{Text A}} -->
<!--                               {\left(\frac{n+1}{\text{total}+1}\right)_\text{Text B}}$$ -->


              
                              


### 3.3.1 로그 오즈비 구하기

앞에서 구한 `odds_ratio`를 `log()`에 적용하면 로그 오즈비를 구할 수 있습니다.
```{r}
frequency_wide <- frequency_wide %>%
  mutate(log_odds_ratio = log(odds_ratio))
```

`log_odds_ratio`의 부호와 크기를 보면 각 단어가 어떤 연설문에서 더 중요하게 사용됐는지 알 수 있습니다. 단어의 `log_odds_ratio`가 0보다 큰 양수일수록 `"moon"`에서 비중이 크고, 반대로 0보다 작은 음수일수록 `"park"`에서 비중이 크다는 것을 의미합니다. 또한 0에 가까울수록 두 연설문의 비중이 비슷함을 의미합니다.

```{r}
# moon에서 비중이 큰 단어
frequency_wide %>%
  arrange(-log_odds_ratio)
```

```{r}
# park에서 비중이 큰 단어
frequency_wide %>%
  arrange(log_odds_ratio)
```

```{r}
# 비중이 비슷한 단어
frequency_wide %>%
  arrange(abs(log_odds_ratio))
```


#### 로그 오즈비 간단히 구하기

로그 오즈비 변수만 필요하다면 다음과 같이 간단히 만들 수 있습니다.

```{r, eval = F}
frequency_wide <- frequency_wide %>%
  mutate(log_odds_ratio = log(((moon + 1) / (sum(moon + 1))) /
                              ((park + 1) / (sum(park + 1)))))
```


### 3.3.2 상대적으로 중요한 단어 비교하기

이제 로그 오즈비 변수를 이용해 각 연설문에서 상대적으로 중요한 단어를 10개씩 추출하겠습니다. 우선 `group_by()`와 `ifelse()`를 이용해 `log_odds_ratio`가 0보다 크면 `"moon"`, 그렇지 않으면 `"park"`을 부여한 `president` 변수를 만들어 항목 별로 분리하겠습니다. 그런 다음, `slice_max()`와 `abs()`를 이용해 `log_odds_ratio`의 절대값 기준으로 상위 10개 단어를 추출하겠습니다. 이렇게 하면 `"moon"`에서 `log_odds_ratio`가 가장 큰 단어 10개, `"park"`에서 `log_odds_ratio`가 가장 작은 단어 10개를 추출합니다. 출력 결과를 보면 **3.2.3**에서 오즈비 `odds_ratio`를 이용해 상하위 10개 단어를 추출했을 때와 같다는 것을 알 수 있습니다.

```{r}
top10 <- frequency_wide %>%
  group_by(president = ifelse(log_odds_ratio > 0, "moon", "park")) %>%
  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)
```


<!-- ```{r eval=F} -->
<!-- top10 %>%  -->
<!--   arrange(-log_odds_ratio) -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- top10 %>%  -->
<!--   arrange(-log_odds_ratio) %>%  -->
<!--   print(n = 10) -->
<!-- ``` -->


```{r}
top10 %>% 
  arrange(-log_odds_ratio) %>% 
  select(word, log_odds_ratio, president) %>% 
  print(n = Inf)
```

<br>

> [편집] 줄맞춤

<!-- 이제 로그 오즈비 변수를 이용해 각 연설문에서 상대적으로 중요한 단어를 10개씩 추출하겠습니다. 우선 `filter()`와 `rank()`를 이용해 `log_odds_ratio`의 순위가 상하위 10위에 드는 단어를 추출하겠습니다. 그런 다음, 각 단어가 어떤 연설문에서 더 중요한지 알 수 있도록 `log_odds_ratio`가 0보다 크면 `"moon"`, 그렇지 않으면 `"park"`을 부여한 `president` 변수를 추가하겠습니다. 다음 코드로 추출한 단어를 보면 앞에서 오즈비 `odds_ratio`를 이용해 상하위 10개 단어를 추출했을 때와 같다는 것을 알 수 있습니다. -->

<!-- ```{r} -->
<!-- top10 <- frequency_wide %>% -->
<!--   filter(rank(log_odds_ratio) <= 10 | rank(-log_odds_ratio) <= 10) %>% -->
<!--   mutate(president = ifelse(log_odds_ratio > 0, "moon", "park")) -->
<!-- ``` -->


<!-- ```{r eval=F} -->
<!-- top10 %>% -->
<!--   arrange(-log_odds_ratio) -->
<!-- ``` -->


<!-- ```{r echo=F} -->
<!-- top10 %>% -->
<!--   arrange(-log_odds_ratio) %>% print(n = 10) -->
<!-- ``` -->


### 3.3.3 막대 그래프 만들기

이제 앞에서 만든 데이터를 이용해 막대 그래프를 만들겠습니다. 그래프를 보면 각 단어가 어떤 연설문에서 중요한지에 따라 서로 다른 축 방향으로 표현됩니다. 이처럼 로그 오즈비로 막대 그래프를 만들면 텍스트의 차이를 잘 드러낼 수 있습니다.

```{r}
ggplot(top10, aes(x = reorder(word, log_odds_ratio),
                  y = log_odds_ratio,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))
```


## 3.4 TF-IDF - 여러 텍스트의 단어 비교하기

오즈비는 어떤 사건이 두 조건하에서 발생할 확률을 이용해 계산하기 때문에 3개 이상의 텍스트를 비교할 때는 적절하지 않습니다. 텍스트를 둘씩 짝지어 따로따로 비교하는 방법도 있지만 텍스트의 수가 많으면 계산 절차가 길고 결과를 해석하기 어렵기 때문에 효율적이지 않습니다.


#### 중요한 단어란 무엇일까?

셋 이상의 텍스트를 비교하는 가장 쉬운 방법은 각 텍스트에서 많이 사용된 단어를 알아보는 것입니다. 하지만 앞에서 살펴보았듯이 이런 단어는 누구나 많이 사용하는 흔한 단어이기 때문에 중요하다고 보기 어렵습니다. 예를 들어 대부분의 자기소개서에 "저는"이라는 단어가 많이 나오겠지만 이 단어를 중요하다고 할 수는 없습니다.

중요한 단어는 '흔하지 않으면서도 특정 텍스트에서는 자주 사용되는 단어'라고 할 수 있습니다. 이런 단어는 특정 텍스트가 다른 텍스트와 구별되는 특징, 개성을 드러냅니다. 예를 들어 어떤 자기소개서에 "스카이다이빙"이라는 흔하지 않은 단어가 여러 번 사용됐다면, 이 단어는 글쓴이의 개성을 잘 드러낸다고 볼 수 있습니다.


#### TF-IDF

**TF-IDF(Term Frequency - Inverse Document Frequency)**는 어떤 단어가 '흔하지 않으면서도 특정 텍스트에서는 자주 사용된 정도'를 나타낸 지표입니다. TF-IDF를 구하면 텍스트의 개성을 드러내는 주요 단어를 찾을 수 있습니다. 계산 과정을 살펴보면서 TF-IDF의 의미를 알아보겠습니다.

> [참고] TF-IDF는 우리말로 '단어 빈도-역문서 빈도'라고 합니다.

#### TF

TF-IDF에서 TF는 텍스트에 단어가 사용된 횟수, **단어 빈도(Term Frequency)**를 의미합니다. 다음 표의 숫자는 자기소개서별로 각 단어가 사용된 횟수, TF를 의미합니다.

  ----------------------------------------------------------
   단어          자기소개서 A   자기소개서 B    자기소개서 C
  ------------- ------------- -------------  ---------------
  저는           15            10             10

  스카이다이빙   3             0              0

  자기주도적     3             5              3

  데이터         0             5              1

  배낭여행       2             3              5
  ----------------------------------------------------------


#### DF와 IDF

**DF(Document Frequency)**는 단어가 몇 개의 텍스트에 사용됐는지 나타낸 **문서 빈도**입니다. 단어의 DF가 클수록 여러 문서에 흔하게 사용된 일반적인 단어라고 할 수 있습니다.

DF를 이용해 IDF를 구할 수 있습니다. **IDF(Inverse Document Frequency)**는 전체 문서 수(N)에서 DF가 차지하는 비중을 구한 다음, 역수를 취해 로그를 취한 값입니다. 우리 말로는 **역 문서 빈도**라고 합니다.

<!-- $$\text{IDF} = \log{(\frac{{\text{N}}}{{\text{DF}}})}$$ -->

<!-- $$\text{IDF} = \log{\Big(\frac{{\text{N}}}{{\text{DF}}}\Big)}$$ -->
<!-- $$\text{IDF} = \log{\Bigg(\frac{{\text{N}}}{{\text{DF}}}\Bigg)}$$ -->

$$\text{IDF} = \log{\frac{{\text{N}}}{{\text{DF}}}}$$



IDF는 DF의 역수이므로 DF가 클수록 작아지고 반대로 DF가 작을수록 커집니다. 따라서 IDF가 클수록 드물게 사용되는 특이한 단어, IDF가 작을수록 흔하게 사용되는 일반적인 단어라고 할 수 있습니다.

다음 표를 보면 "스카이다이빙"을 사용한 자기소개서는 1개 밖에 없기 때문에 IDF가 1.1로 높습니다. 따라서 "스카이다이빙"은 흔하지 않은 단어라고 볼 수 있습니다. 반면 "저는", "자기주도적", "배낭여행"은 모든 자기소개서에 사용되어 IDF가 0이므로 흔한 단어라고 볼 수 있습니다.


  -----------------------------------------
   단어           DF             IDF                    
  ------------- ------------- -------------
  저는           3             $$log\frac{3}{3} = 0$$ 

  스카이다이빙   1             $$log\frac{3}{1} = 1.1$$ 

  자기주도적     3             $$log\frac{3}{3} = 0$$ 

  데이터         2             $$log\frac{3}{2} = 0.4$$ 

  배낭여행       3             $$log\frac{3}{3} = 0$$ 
  -----------------------------------------

> [참고] 소수점 둘째 자리에서 반올림하여 표기하였습니다.

<br>

> [편집] 수식 줄 맞춤
  
<!--   ----------------------------------------- -->
<!--    단어           DF             IDF                     -->
<!--   ------------- ------------- ------------- -->
<!--   저는           3             $$log(\frac{3}{3}) = 0$$  -->

<!--   스카이다이빙   1             $$log(\frac{3}{1}) = 1.1$$  -->

<!--   자기주도적     3             $$log(\frac{3}{3}) = 0$$  -->

<!--   데이터         2             $$log(\frac{3}{2}) = 0.4$$  -->

<!--   배낭여행       3             $$log(\frac{3}{3}) = 0$$  -->
<!--   ----------------------------------------- -->


#### TF-IDF

TF-IDF는 TF(단어 빈도)와 IDF(역문서 빈도)를 곱한 값입니다. TF-IDF는 어떤 단어가 분석 대상이 되는 텍스트 내에서 많이 사용될 수록 커지고(TF), 동시에 해당 단어가 사용된 텍스트가 드물수록 커지는(IDF) 특성을 지닙니다. 즉, '흔하지 않은 단어인데 특정 텍스트에서 자주 사용될수록' 큰 값을 지닙니다. 그러므로 각 텍스트에서 TF-IDF가 큰 단어를 살펴 보면 다른 텍스트와 구별되는 특징을 알 수 있습니다.

<!-- $$\text{TF-IDF} = TF{\times}\log{(\frac{{\text{N}}}{{\text{DF}}})}$$ -->

<!-- $$\text{TF-IDF} = TF{\times}\log{\Big(\frac{{\text{N}}}{{\text{DF}}}\Big)}$$ -->

$$\text{TF-IDF} = TF{\times}\log\frac{{\text{N}}}{{\text{DF}}}$$


단어 빈도만 보면 어떤 자기소개서든 가장 많이 사용된 단어는 "저는"입니다. 하지만 다음 표를 보면 자기소개서 A에서 TF-IDF가 가장 높은 단어는 "스카이다이빙"입니다. 따라서 "저는"이 아니라 "스카이다이빙"이 자기소개서 A의 특징을 가장 잘 드러내는 단어라고 할 수 있습니다.

자기소개서 B와 C는 둘 다 "데이터"의 TF-IDF가 가장 높아 텍스트의 특징을 가장 잘 드러낸다고 할 수 있습니다. 그런데 자기소개서 B는 "데이터"를 5번 사용해 TF-IDF가 2로 높지만, 자기소개서 C는 1번 사용해 TF-IDF가 0.4로 낮습니다. 따라서 "데이터"는 자기소개서 B의 특징을 더 잘 드러내는 단어라고 할 수 있습니다.

  <!-- ---------------------------------------------------------------------------------------------------------------------- -->
  <!--  단어          자기소개서 A                        자기소개서 B                        자기소개서 C -->
  <!-- ------------- ----------                         -------------                      -------------- -->
  <!-- 저는           $$15{\times}\log(\frac{3}{3})=0$$   $$10{\times}\log(\frac{3}{3})=0$$   $$10{\times}\log(\frac{3}{3})=0$$ -->

  <!-- 스카이다이빙   $$3{\times}\log(\frac{3}{1})=3.3$$  $$0{\times}\log(\frac{3}{1})=0$$    $$0{\times}\log(\frac{3}{1})=0$$ -->

  <!-- 자기주도적     $$3{\times}\log(\frac{3}{3})=0$$    $$5{\times}\log(\frac{3}{3})=0$$    $$3{\times}\log(\frac{3}{3})=0$$ -->

  <!-- 데이터         $$0{\times}\log(\frac{3}{2})=0$$    $$5{\times}\log(\frac{3}{2})=2$$    $$1{\times}\log(\frac{3}{2})=0.4$$ -->

  <!-- 배낭여행       $$2{\times}\log(\frac{3}{3})=0$$    $$3{\times}\log(\frac{3}{3})=0$$    $$5{\times}\log(\frac{3}{3})=0$$ -->
  <!-- ---------------------------------------------------------------------------------------------------------------------- -->




  ----------------------------------------------------------------------------------------------------------------------
   단어          자기소개서 A                        자기소개서 B                        자기소개서 C
  ------------- ----------                        -------------                    --------------
  저는           $$15{\times}\log\frac{3}{3}=0$$   $$10{\times}\log\frac{3}{3}=0$$   $$10{\times}\log\frac{3}{3}=0$$

  스카이다이빙   $$3{\times}\log\frac{3}{1}=3.3$$  $$0{\times}\log\frac{3}{1}=0$$    $$0{\times}\log\frac{3}{1}=0$$

  자기주도적     $$3{\times}\log\frac{3}{3}=0$$    $$5{\times}\log\frac{3}{3}=0$$    $$3{\times}\log\frac{3}{3}=0$$

  데이터         $$0{\times}\log\frac{3}{2}=0$$    $$5{\times}\log\frac{3}{2}=2$$    $$1{\times}\log\frac{3}{2}=0.4$$

  배낭여행       $$2{\times}\log\frac{3}{3}=0$$    $$3{\times}\log\frac{3}{3}=0$$    $$5{\times}\log\frac{3}{3}=0$$
  ----------------------------------------------------------------------------------------------------------------------

> [참고] 소수점 둘째 자리에서 반올림하여 표기하였습니다.

<br>


> [편집] 각 자기소개서 중 TF-IDF 높은 셀 강조하기, 수식 줄 맞춤


### 3.4.1 단어 빈도 구하기

TF-IDF의 원리를 살펴봤으니 텍스트를 분석하는데 활용해보겠습니다. 먼저 역대 대통령의 대선 출마 선언문을 담은 `speeches_presidents.csv`를 불러와 기본적인 전처리를 한 다음, 명사를 추출해 단어 빈도를 구하겠습니다. CSV 파일을 불러올 때는 `readr` 패키지의 `read_csv()`를 이용하겠습니다. `read_csv()`는 데이터를 다루기 편한 tibble 구조로 만들어주고 `read.csv()`에 비해 속도가 더 빠릅니다.


```{r echo=F}
options(tibble.width = 47)      # tibble 출력 폭 제한
```


```{r eval=F}
# 데이터 불러오기
install.packages("readr")
library(readr)

raw_speeches <- read_csv("speeches_presidents.csv")
raw_speeches
```

```{r echo=F}
# install.packages("readr")
library(readr)
raw_speeches <- read_csv(here::here("files/speeches_presidents.csv"))
raw_speeches
```


```{r}
# 기본적인 전처리
speeches <- raw_speeches %>%
  mutate(value = str_replace_all(value, "[^가-힣]", " "),
         value = str_squish(value))

# 토큰화
speeches <- speeches %>%
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)

# 단어 빈도 구하기
frequecy <- speeches %>%
  count(president, word) %>%
  filter(str_count(word) > 1)

frequecy
```



```{r echo=F}
options(tibble.width = 80)      # tibble 출력 폭 제한
```

### 3.4.2 TF-IDF 구하기

`tidytext`패키지의 `bind_tf_idf()`를 이용하면 TF-IDF를 구할 수 있습니다. `bind_tf_idf()`에는 세 가지 파라미터를 입력해야 합니다.
- `term`     : 단어
- `document` : 텍스트 구분 변수
- `n`        : 단어 빈도

앞에서 만든 `frequecy`를 `bind_tf_idf()`에 적용해 TF-IDF를 구한 다음, `tf_idf`가 높은 순으로 정렬하겠습니다. 출력 결과를 보면 `tf`, `idf`, `tf_idf`가 추가되었음을 확인할 수 있습니다.

```{r}
frequecy <- frequecy %>%
  bind_tf_idf(term = word,           # 단어
              document = president,  # 텍스트 구분 변수
              n = n) %>%             # 단어 빈도
  arrange(-tf_idf)

frequecy
```

> [참고] bind_tf_idf()로 생성한 `tf`는 앞에서 설명한 tf와 달리 대상 텍스트의 전체 단어 수에서 해당 단어의 수가 차지하는 '비중'을 의미합니다. 단어 빈도를 전체 단어 빈도로 나눈 '비율'이므로 텍스트에 사용된 전체 단어 수가 많을수록 작아집니다.


#### TF-IDF가 높은 단어 살펴보기

TF-IDF를 구했으니 이제 텍스트의 특징을 드러내는 중요한 단어가 무엇인지 쉽게 파악할 수 있습니다. 살펴볼 텍스트를 추출해 `tf_idf`가 높은 단어를 살펴보면 각 대통령이 다른 대통령들과 달리 무엇을 강조했는지 알 수 있습니다.

```{r echo=F}
options(
  tibble.print_min = 5,  # 행 출력 제한
  tibble.print_max = 5)  # 행 출력 제한
```


```{r}
frequecy %>% filter(president == "문재인")
```

<br>

```{r}
frequecy %>% filter(president == "박근혜")
```

> [편집] 2단 편집

<br>

```{r}

frequecy %>% filter(president == "이명박")
```

<br>

```{r}

frequecy %>% filter(president == "노무현")
```

> [편집] 2단 편집


#### TF-IDF가 낮은 단어 살펴보기

반대로 TF-IDF가 낮은 단어를 살펴보면, 역대 대통령들이 공통적으로 사용한 흔한 단어를 알 수 있습니다. 출력 결과를 보면 "국민", "경제" 등 범용적인 단어의 TF-IDF가 0임을 알 수 있습니다.

```{r}
frequecy %>%
  filter(president == "문재인") %>%
  arrange(tf_idf)

frequecy %>%
  filter(president == "박근혜") %>%
  arrange(tf_idf)
```


### 3.4.3 막대 그래프 만들기

각 연설문에서 TF-IDF가 높은 단어를 추출해 막대 그래프를 만들겠습니다. 출력한 그래프를 보면 각 대통령의 개성을 드러내는 단어가 무엇인지 쉽게 파악할 수 있습니다.

```{r}
# 주요 단어 추출
top10 <- frequecy %>%
  group_by(president) %>%
  slice_max(tf_idf, n = 10, with_ties = F)

# 그래프 순서 정하기
top10$president <- factor(top10$president,
                          levels = c("문재인", "박근혜", "이명박", "노무현"))

# 막대 그래프 만들기
ggplot(top10, aes(x = reorder_within(word, tf_idf, president),
                  y = tf_idf,
                  fill = president)) +  
  geom_col(show.legend = F) +
  coord_flip() +
  facet_wrap(~ president, scales = "free", ncol = 2) +
  scale_x_reordered() +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))
```

> [참고] `factor()`를 이용해 변수 항목의 `levles`를 정하면 원하는 순서로 그래프를 나열할 수 있습니다.

<br>

> [알아두면 좋아요] TF-IDF의 한계와 대안

모든 문서에 사용된 단어는 IDF가 0이므로 TF-IDF도 0이 됩니다. 따라서 TF-IDF를 활용하면 단어가 특정 문서에서 특출나게 많이 사용됐더라도 발견할 수 없다는 한계가 있습니다. **Weighted log odds**를 활용하면 이런 한계를 극복할 수 있습니다. Weighted log odds는 단어 등장 확률로 가중치를 부여하기 때문에 모든 문서에 사용됐지만 특정 문서에 많이 사용된 단어를 발견할 수 있습니다. 또한 오즈비와 달리 셋 이상의 문서를 비교할 때도 사용할 수 있다는 장점이 있습니다. `tidylo` 패키지를 이용하면 Weighted log odds를 쉽게 구할 수 있습니다.

- tidylo: Weighted Tidy Log Odds Ratio
  [github.com/juliasilge/tidylo](https://github.com/juliasilge/tidylo)




